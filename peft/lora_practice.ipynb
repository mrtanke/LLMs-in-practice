{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7034220e-6bde-418e-8583-74adb0d5f6bb",
   "metadata": {},
   "source": [
    "# LoRA Practice\n",
    "* **Base model**: `bigscience/bloom-7b1`\n",
    "* **LoRA fine-tuning for BLOOM**:\n",
    "  * Implemented in a **plug-and-play (plugin/adapter) style**, meaning the LoRA adapters can be added or removed without modifying the base model structure.\n",
    "  * freeze original weights\n",
    "  * plugin lora adapters (peft)\n",
    "* **Using the Hugging Face `transformers` library**:\n",
    "  * Understanding the parameters and workflow of `trainer.train()` is essential.\n",
    "  * **MLM vs. CLM (training objectives)**:\n",
    "    * Both are **unsupervised learning**, where the model can automatically construct `input/labels` pairs.\n",
    "    * **MLM (Masked Language Modeling)**: Used by models like **BERT**,\n",
    "        * where some tokens are masked and the model predicts them.\n",
    "    * **CLM (Causal Language Modeling)**: Used by models like **GPT/BLOOM**,\n",
    "        * where the model predicts the next token given the previous context.\n",
    "* **Training & Inference Pipeline**:\n",
    "  1. **Dataset & Tasks** – prepare datasets aligned with the desired objective.\n",
    "  2. **Tokenizer** – preprocess input text into tokens consistent with the base model’s vocabulary.\n",
    "  3. **Training** – fine-tune the base model with LoRA adapters using PEFT, while keeping frozen parameters intact.\n",
    "  4. **Inference** – load the fine-tuned LoRA model for downstream tasks.\n",
    "\n",
    "The traning and inference whole process:\n",
    "1. Load base model\n",
    "    1. Freeze weights；\n",
    "2. Configure Lora Adapter\n",
    "    1. Configure Lora config\n",
    "    2. Inject to model(get\\_peft\\_model)\n",
    "3. Use Trainer for training\n",
    "    1. Set (model, train_dataset, args)\n",
    "    2. train\n",
    "4. Save and load Lora weights\n",
    "5. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812b0e4-dbe2-4c65-a0df-a695c41f091f",
   "metadata": {},
   "source": [
    "## 1. base model & lora adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a0556-317c-49e3-9937-98d7e18661fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM # from huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9842e-42df-4dbf-8f41-9a001918a2bf",
   "metadata": {},
   "source": [
    "## 1.1 load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f512c27-c32b-438e-81da-ef2990856cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloom-7b1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81deeaf-1136-4d9f-9467-7765deaa5f17",
   "metadata": {},
   "source": [
    "## 1.2 freeze original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae0bb2a-376a-4f6d-bcdf-a7189faa20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(model.parameters()):\n",
    "    param.requires_grad = False # freeze the model, train adapters later\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters(e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32)\n",
    "\n",
    "# reduce number of stored activations\n",
    "# Save video memory at the expense of time\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48043b6-1851-494d-875f-e7c385278a1c",
   "metadata": {},
   "source": [
    "## 1.3 Lora Adapters\n",
    "lora Adapters -> ΔW = B·A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996256f-afe6-46b2-8632-3ab5aa24f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, # Low rank r, determines the intermediate dimension of A/B\n",
    "    lora_alpha=32, # Scaling factor, multiply by (alpha / r) on output\n",
    "    lora_dropout=0.05, # Dropout during LoRA insertion\n",
    "    bias=\"none\", # # No handle to the bias parameter\n",
    "    task_type=\"CAUSAL_LM\" # Task type, set this for CLM or Seq2Seq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ced39-35b0-4d6c-ad41-128fa66ee9bd",
   "metadata": {},
   "source": [
    "```python\n",
    "def lora_forward_matmul(x, W, W_A, W_B):\n",
    "    h = x @ W   # regular matrix multiplication\n",
    "    h += x @ (W_A @ W_B) * alpha   # use scaled LoRA weights\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ccea0-ade3-4fb7-a8e8-a3bc04604cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378827e2-4223-4e56-a2d0-5b2c1079fb97",
   "metadata": {},
   "source": [
    "## 2. pipeline\n",
    "### 2.1 data\n",
    "- dataset -> Abirate/english_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d0364-3370-40b2-8400-e59a3b027d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a454e-dcf8-481c-8ec2-19d8a80b5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3db8e0-5788-49ee-a078-a6d91aeeb09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c042e-3bd1-4fad-ba60-9bccdf17b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(row):\n",
    "    row['prediction'] = row['quote'] + ' ->: ' + str(row['tags'])\n",
    "    return row\n",
    "dataset['train'] = dataset['train'].map(merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbfe56-18a7-4abd-bd88-c1f692854a1e",
   "metadata": {},
   "source": [
    "### 2.2 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98371dc9-99d6-4089-a25c-933df846709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda samples: tokenizer(samples['prediction']), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a45d4-e558-4d69-983a-15cecad66803",
   "metadata": {},
   "source": [
    "### 2.3 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb70494-65d6-4346-94af-c9e73eacc897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    args=TrainingArguments( # Training hyperparameters\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_steps=200,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # datasets -> batch(input_ids, labels)\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.tran()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298bbb02-901d-4794-b5f5-212ed7a5b509",
   "metadata": {},
   "source": [
    "### 2.4 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a95831-0dfe-4172-814d-69c0ca2ffb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(\"“An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains.” ->: \", return_tensors='pt')\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(**batch, max_new_tokens=50)\n",
    "\n",
    "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (bert)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
