{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b22cbe-40f7-4563-8105-df86994e7e3e",
   "metadata": {},
   "source": [
    "# LLaMA（Large Language Model Meta AI）\n",
    "\n",
    "## 1. What is LLaMA?\n",
    "- **LLaMA** = Large Language Model Meta AI\n",
    "- Goal: Provide **smaller, efficient open models** comparable to GPT-3/4\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Versions\n",
    "### Pure pre-trained language model:\n",
    "- **LLaMA 1** (2023.02): 7B, 13B, 33B, 65B\\\n",
    "  → **Alpaca** 7B (instruction-tuned in Instruction-Following Dataset from GPT-3)\\\n",
    "  → **Vicuna** 13B, 33B (fine-tuned for dialogue)\n",
    "- **LLaMA 2** (2023.07): 7B, 13B, 70B\\\n",
    "  → **LLaMA-2-Chat** (instruction-tuned for dialogue)\n",
    "- **LLaMA 3** (2024.04): 8B, 70B\n",
    "- **LLaMA 4** (2025.04): 17B + 109B, 400B, 288B + 2T  \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Architecture\n",
    "- **Decoder-only Transformer** (like GPT)\n",
    "- Training objective: **Autoregressive LM** (next-token prediction)\n",
    "- Key improvements:\n",
    "  - **SwiGLU** activation (faster & more stable than ReLU/GELU)\n",
    "  - **RMSNorm** instead of LayerNorm\n",
    "  - **RoPE (Rotary Position Embedding)** for relative position encoding\n",
    "  - **Efficient tokenizer** (SentencePiece, 32k vocab)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Training\n",
    "- Dataset: ≈ **1.4T tokens**, high-quality mix (Wikipedia, books, CommonCrawl, code, etc.)\n",
    "- Emphasis on **quality over raw size** (smaller but cleaner than GPT-3’s 45TB)\n",
    "- Techniques: distributed training (ZeRO), mixed precision (FP16/FP32)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Why LLaMA Matters\n",
    "- **Efficiency**: LLaMA-13B matches or beats GPT-3 (175B) on many benchmarks\n",
    "- **Accessibility**: weights available for research & commercial use (under license)\n",
    "- **Ecosystem**: inspired many open-source derivatives (Alpaca, Vicuna, WizardLM, etc.)\n",
    "- **Runs locally**: 7B and 13B can run on a single GPU (e.g., RTX 3090/4090)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Pros and Cons\n",
    "✅ Pros\n",
    "- Smaller but powerful\n",
    "- Open access → strong open-source ecosystem\n",
    "- Easy to fine-tune (LoRA, QLoRA)\n",
    "- Efficient inference\n",
    "\n",
    "❌ Cons\n",
    "- Weaker performance in Chinese & low-resource languages\n",
    "- Still behind GPT-4 in reasoning and multi-modal tasks\n",
    "- LLaMA 1/2/3/4 require license approval (not pure open-source)\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Key Difference from GPT\n",
    "- **Same architecture** (Decoder-only Transformer, autoregressive)\n",
    "- **Different philosophy**:\n",
    "  - GPT: closed, very large-scale, commercial API\n",
    "  - LLaMA: smaller, efficient, open for research/innovation\n",
    "- GPT = “product-first”;  LLaMA = “research foundation”\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Summary\n",
    "- LLaMA = Meta’s efficient, open alternative to GPT\n",
    "- Architecture: Decoder-only Transformer\n",
    "- Impact: boosted open-source NLP, made LLMs more accessible\n",
    "- -> **“Small but mighty open LLM from Meta”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed3de60-25a1-4fc9-9512-87a70940d534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (bert)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
